HDFS
	通常用来读取小型文件
	分为NameNode 和DataNode
	1.NameNode(NN)
		1)Main Function: 接受客户端的读写服务receive reading and writing service from client 
		2)保存文件的元数据信息(Metadate)
			-1 文件owership和permissions(保存在磁盘中)
			-2 文件包含哪些块(保存在磁盘中)
			-3 Block保存在哪个DataNode(数据不保存在NameNode的磁盘上，由DataNode启动时上报)
		3)Metadate启动后会加载到内存
			-1 metadata存储到磁盘的文件名为fsimage
			-2 Block的位置信息不会保存到fsimage
			-3 edits记录对metadata的操作的日志
			-4 新增文件会保存信息在内存和磁盘中
			-5 删除数据的时候不会直接保存到fsimage，而是记录到edits中，
					定期保存信息到fsimage中
	2.Secondary NameNode(SNN)
		1)不是NN的备份（但可以做备份）
		2)主要功能是帮助NN合并edits log，减少NN启动时间
		3)SNN执行合并时机
			-1 根据配置文件设置的时间间隔fs.checkpoint.period
				默认3600秒
			-2 根据配置文件设置edits log大小fs.checkpoint.size规定edits文件的最大值
				默认64MB，如果文件edits log超过64MB则直接进行合并操作
	3.DataNode
		-存储信息，包含数据存储单元（Block）
			1)文件被切分成固定大小的数据块
				-1 默认数据块64MB，可以配置
				-2 文件大小不够64MB，则单独存成一个64MB的block
			3)一个文件的存储方式
				-1 按大小被切分成若干个block，存储到不同节点上
				-2 默认每个block都有三个副本
			4)Block大小和副本(Node)数通过Client端上传文件的时候设置，
				文件上传成功后，副本(Node)数可以更改，Block Size不可变更
			5)启动时汇报block信息给NN
		-通过主动向NN发送心跳保持与其联系(3秒一次)， 如果NN 10分总没有收到DN心跳，
			则认为其已经lost，并copy其上面的block到其他的DN
	4.Block的副本放置策略
		1) first replica(副本): 放置上传文件的DN；
			如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点
		2) second replica:放在于第一个副本不同的机架(Rack)的节点上
				(一个机架Rack使用一个电源，同一个机架传输通常使用同一个交换机
					所以同一机架速度会快，第三个副本会放置与第二个同一个机架上)
		3) third replica: 放置于第二个副本相同的机架节点上
		4) more replica: 随机节点
	5.HDFS文件权限
		1)文件上传者就是文件的拥有者 r:read; w:write; x:execute;
			x对于文件忽略，对文件夹表示是否允许访问其内容
	6.安全模式
		NameNode启动之后就进入安全模式，该模式只允许查看目录，不允许读写操作。
		本阶段DataNode会汇报数据地址，NameNode会检查数据副本数量
		副本数量不够的时候会等待DataNode复制副本完毕
		强制离开本模式会导致数据丢失
		
MR(MapReduce) 分布式计算框架
	- 适合离线计算
	- 移动计算，而不是移动数据
	- 把分析计算的程序分别拷贝到不同的计算机上
	步骤：
		Input -->
		1) Splitting （把数据块，切成片段）-->
								- max.split(100M)
								- min.split(10M)
								- block(64M)
								- max(min.split, min(max.split, block)) = 64M 碎片切分算法
		2) Mapping (键值对，一般是值（单词）做为key，1作为value) --> 
		3) Shuffling (把Mapper的输出按照某种key值重新切分和组合成n份，
								把key值符合某种范围的输出送到特定的reducer，
								也可以简化reducer的过程) --> 
								- 每一个map task都有一个内存缓冲区（默认100M），存储map输出结果
								- 当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，（Spill to disk）
								- 溢写是由单独线程来完成，不影响往缓冲区写map结果的线程（spill.percent, 默认0.8）
								- 当溢写线程结束后，需要对这80MB空间的key做排序（Sort，默认按照ask码的规则(字典规则)来进行比较）
								- Sort之后如果需要进行combiner则执行，否则直接掠过
									= 如果client设置了Combiner，则会将有相同key的键值对的value加起来，
											减少溢写到磁盘的数据量
									=  当整个map task结束后再对磁盘中的这个map task产生的所有临时文件
											做合并（Merge），这个过程类似于找到数据中key值相同的，对其value
											进行合并相加，来减少数据传输
									= reduce 从tasktracker copy数据
									= copy过来的数据先放入内存缓冲区，这里的缓冲区大小要比map端更灵活，
											它基于JVM的heap size 设置
									= merge有三种形式：1）内存到内存，2）内存到磁盘，3）磁盘到磁盘，
											merge从不同的tasktracker上拿到数据
		4) Reducing  （根据key相同进行合并）-->
			FinalResult
MapReduce的架构
	- 一主多从架构
	- 主JobTracker(1.0之后的版本中有，在2.0就没了):
		负责调度分配每一个子任务task运行于TaskTracker上，如果发现有失败的task，
			就重新分配其任务到其他节点。每个hadoop集群中只有一个JobTracker，
			一般运行在Master节点上，但是可以指定任何一台机器进行部署
	- 从TaskTracker
		= TaskTracker主动与JobTracker通信，接受作业，并负责直接执行每一个任务。
			为了减少网络带宽，TaskTracker最好运行在HDFS的DataNode上
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	